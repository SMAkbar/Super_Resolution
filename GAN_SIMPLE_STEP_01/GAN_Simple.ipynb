{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMPLE GENERATIVE ADVERSIAL NETWORK\n",
    "### By: Syed Muhammad Akbar\n",
    "@Reference: https://www.tensorflow.org/tutorials/generative/dcgan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Tensorflow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional: Run the following if the imageio is not installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# To generate GIFs\n",
    "!pip3 install -q imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD and Prepare MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 7s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f9803a01d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00204767]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as we go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 2 is 409.54809641838074 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-d152560ca122>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-802af7bf198a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m       \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Produce images for the GIF as we go\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\try01\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    402\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\try01\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\try01\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    587\u001b[0m     \"\"\"\n\u001b[0;32m    588\u001b[0m     return self._call_flat(\n\u001b[1;32m--> 589\u001b[1;33m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[0;32m    590\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m    591\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\try01\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\try01\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[0;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[1;32m--> 445\u001b[1;33m             ctx=ctx)\n\u001b[0m\u001b[0;32m    446\u001b[0m       \u001b[1;31m# Replace empty list with None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\try01\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x1f9a14f0198>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'image_at_epoch_0050.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-7d0477526f8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplay_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-2f3d8de0ec11>\u001b[0m in \u001b[0;36mdisplay_image\u001b[1;34m(epoch_no)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Display a single image using the epoch number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdisplay_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'image_at_epoch_{:04d}.png'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\envs\\try01\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2769\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2770\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2771\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'image_at_epoch_0050.png'"
     ]
    }
   ],
   "source": [
    "display_image(EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated GIF saved during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  last = -1\n",
    "  for i,filename in enumerate(filenames):\n",
    "    frame = 2*(i**0.5)\n",
    "    if round(frame) > round(last):\n",
    "      last = frame\n",
    "    else:\n",
    "      continue\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "\n",
    "import IPython\n",
    "if IPython.version_info > (6,2,0,''):\n",
    "  display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "   pass\n",
    "else:\n",
    "  files.download(anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
